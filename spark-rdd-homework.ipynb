{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HackerNews data analysis challenge with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will analyse a dataset of (almost) all submitted HackerNews posts with Spark. \n",
    "\n",
    "The notebook is a convenient way to explore your data, but you will be asked to implement corresponding functions in the `spark_rdd.py` file that will be submitted to KATE. Having standalone functions is a good way to automate reporting of the different metrics we will look at in a prouction environment.\n",
    "\n",
    "Let's start by importing some of the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "DATASET_URL = \"https://s3-eu-west-1.amazonaws.com/kate-datasets/hackernews/HNStories.zip\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    req = urllib.request.urlopen(DATASET_URL)\n",
    "    data = BytesIO(req.read())\n",
    "\n",
    "    with zipfile.ZipFile(data, \"r\") as zipref:\n",
    "        zipref.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "Ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:57979)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:57979)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:57979)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rosie\\Miniconda3\\envs\\ads10\\lib\\site-packages\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "from datetime import datetime as dt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "print(\"Ready to go!\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has one JSON entry per line. In order to make accessing it easier, first turn each entry as a dictionary and use `persist()` to cache the resulting RDD. \n",
    "\n",
    "Remember to download the dataset (instructions in `Readme.md`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_json = sc.textFile(\"data/HNStories.json\")\n",
    "dataset = dataset_json.map(lambda x: json.loads(x))\n",
    "dataset.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour(rec):\n",
    "    return re.compile(r\"\\w+\").findall(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time(timestamp):\n",
    "    return dt.utcfromtimestamp(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket(rec, min_timestamp, max_timestamp):\n",
    "    interval = (max_timestamp - min_timestamp + 1) /200.0\n",
    "    return int((rec[\"created_at_i\"] - min_timestamp) / interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Spark has many helper functions on top of the ones we have studied which you will find useful. You can view them at [http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `count_elements_in_dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with some initial analysis. \n",
    "\n",
    "**How many elements are in your datasets?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements_in_dataset(dataset):\n",
    "    return dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1333789"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_elements_in_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_first_element`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the first element look like? Assign the result to a variable called `first` **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_element(dataset):\n",
    "    return dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'TuxLyn',\n",
       " 'created_at': '2014-05-29T08:25:40Z',\n",
       " 'created_at_i': 1401351940,\n",
       " 'num_comments': 0,\n",
       " 'objectID': '7815290',\n",
       " 'points': 1,\n",
       " 'title': 'DuckDuckGo Settings',\n",
       " 'url': 'https://duckduckgo.com/settings'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_first_element(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_all_attributes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element is a dictionary of attributes and their values for a post. \n",
    "**Can you find the set of all attributes used throughout the RDD?**\n",
    "The function `dictionary.keys()` gives you the list of attributes of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_attributes(dataset):\n",
    "    all_attributes = dataset.flatMap(lambda x: x.keys()).distinct().collect()\n",
    "    return all_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_at',\n",
       " 'points',\n",
       " 'created_at_i',\n",
       " 'story_text',\n",
       " 'objectID',\n",
       " 'title',\n",
       " 'author',\n",
       " 'url',\n",
       " 'story_id',\n",
       " 'num_comments']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_attributes(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_elements_w_same_attributes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are more attributes than just the one used in the first element. Can you filter the dataset to keep only elements that have the same set of attributes as the first element?\n",
    "Hint: you might want to write a function that compares attributes for two elements and apply it on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_w_same_attributes(dataset):\n",
    "    def compare_elems(first, second):\n",
    "        if len(first) != len(second):\n",
    "            return False\n",
    "        for key in first.keys():\n",
    "            if key not in second:\n",
    "                return False\n",
    "        return True\n",
    "    first = dataset.first()\n",
    "    return dataset.filter(lambda x: compare_elems(first, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements did you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many posts through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field `created_at_i` is very useful, it gives you a UNIX timestamp of the time at which the file was created. The following function lets you extract a time from a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_min_max_timestamps`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the minimum and maximum timestamps in the RDD and call them `min_time` and `max_time`.** \n",
    "These correspond to the first and last post, when did they occur? For the function `get_min_max_timestamps` you need to return min_time and max_time given a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum timestamp\n",
    "def get_min_max_timestamps(dataset):\n",
    "    min_time = dataset.map(lambda x: x[\"created_at_i\"]).reduce(\n",
    "    lambda x, y: x if x < y else y\n",
    "    )\n",
    "    max_time = dataset.map(lambda x: x[\"created_at_i\"]).reduce(\n",
    "    lambda x, y: x if x > y else y\n",
    "    )\n",
    "    return extract_time(min_time), extract_time(max_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_number_of_posts_per_bucket`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets analyse how many elements through time. The following function assigns a record to one of 200 \"buckets\" of time. **Use it to count the number of elements that fall within each bucket and call the result `bucket_rdd`.** The result should be such that `buckets` below generates the corresponding output. If you want to use this function in your `spark_rdd.py` you will need to redefine it in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_posts_per_bucket(dataset, min_time, max_time):\n",
    "    min_timestamp = min_time.timestamp()\n",
    "    max_timestamp = max_time.timestamp()\n",
    "    return dataset.map(lambda x: (get_bucket(x, min_timestamp, max_timestamp), 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = sorted(buckets_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this to plot the number of submitted posts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bs = [dt.utcfromtimestamp(x[0]*interval + min_time) for x in buckets]\n",
    "ts = [x[1] for x in buckets]\n",
    "plt.plot(bs, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_number_of_posts_per_hour`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following function gets the hour of the day at which a post was submitted. **Use it to find the number of posts submitted at each hour of the day.** The value of `hours_buckets` should match the one printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour(rec):\n",
    "    t = dt.utcfromtimestamp(rec['created_at_i'])\n",
    "    return t.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_posts_per_hour(dataset):\n",
    "    return dataset.map(lambda x: (get_hour(x), 1)).reduceByKey(lambda x, y: x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_buckets = sorted(hours_buckets_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hrs = [x[0] for x in hours_buckets]\n",
    "sz = [x[1] for x in hours_buckets]\n",
    "plt.plot(hrs, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_score_per_hour`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of points scored by a post is under the attribute `points`. **Use it to compute the average score received by submissions for each hour.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_per_hour(dataset):\n",
    "    scores_per_hour_rdd = (\n",
    "    data.set.map(lambda x: (get_hour(x), (x[\"points\"], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "    )\n",
    "    \n",
    "    return scores_per_hour_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_hour = sorted(scores_per_hour_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = [x[0] for x in scores_per_hour]\n",
    "sz = [x[1] for x in scores_per_hour]\n",
    "plt.plot(hrs, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_proportion_of_scores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be more useful to look at sucessful posts that get over 200 points. **Find the proportion of posts that get above 200 points per hour.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportion_of_scores(dataset):\n",
    "    prop_per_hour_rdd = (\n",
    "    dataset.map(lambda x: (get_hour(x), (1 if x[\"points\"] > 200 else 0, 1)))\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], y[0], x[1] + y[1]))\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "    )\n",
    "    \n",
    "    return prop_per_hour_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_per_hour = sorted(prop_per_hour_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = [x[0] for x in prop_per_hour]\n",
    "sz = [x[1] for x in prop_per_hour]\n",
    "plt.plot(hrs, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_proportion_of_success`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function lists the word in the title. **Use it to count the number of words in the title of each post, and look at the proportion of successful posts for each title length.**\n",
    "\n",
    "If an element does not have a title, it should count it as a length of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_words(line):\n",
    "    return re.compile('\\w+').findall(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportion_of_success(dataset):\n",
    "    prop_per_title_length_rdd = (\n",
    "    dataset.map(\n",
    "        lambda x: (\n",
    "            len(get_word(x.get(\"title\", \"\"))),\n",
    "        )\n",
    "    )\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "    )\n",
    "    \n",
    "    return prop_per_title_length_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_per_title_length = sorted(prop_per_title_length_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = [x[0] for x in prop_per_title_length]\n",
    "sz = [x[1] for x in prop_per_title_length]\n",
    "plt.plot(hrs, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_title_length_distribution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare this with the distribution of number of words. **Count for each title length the number of submissions with that length.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_length_distribution(dataset):\n",
    "    submissions_per_length_rdd = dataset.map(\n",
    "        lambda x: (len(get_words(x.get(\"title\", \"\"))), 1)\n",
    "    ).reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    return submissions_per_length_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_per_length = sorted(submissions_per_length_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = [x[0] for x in submissions_per_length]\n",
    "sz = [x[1] for x in submissions_per_length]\n",
    "plt.plot(hrs, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most people are getting it wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this task, you will need a new function: `takeOrdered()`. Like `take()` it collects elements from an RDD. However, it can be applied to take the smallest elements. For example, `takeOrdered(10)` returns the 10 smallest elements. Furthermore, you can pass it a function to specify the way in which the elements should be ordered. For example, `takeOrdered(10, lambda x: -x)` will return the 10 largest elements.\n",
    "\n",
    "The function below extracts the url domain out of a record. **Use it to count the number of distinct domains posted to.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def get_domain(rec):\n",
    "    url = urlparse(rec['url']).netloc\n",
    "    if url[0:4] == 'www.':\n",
    "        return url[4:]\n",
    "    else:\n",
    "        return url\n",
    "print(get_domain(dataset.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `takeOrdered()` find the 25 most popular domains posted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(25)\n",
    "labels = [x[0] for x in top25]\n",
    "counts = np.array([x[1] for x in top25]) * 100.0/dataset.count()\n",
    "plt.xticks(index,labels, rotation='vertical')\n",
    "plt.bar(index, counts, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
